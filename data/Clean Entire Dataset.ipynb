{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "# preprocess!\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['rt'])  # remove the retweet tag!\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "import re\n",
    "\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sents(sent):\n",
    "    return ' '.join(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links_and_html(sentence):\n",
    "    sentence = re.sub(r'http\\S+', '', sentence)\n",
    "    sentence = re.sub(r'<[^<]+?>', '', sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def remove_punct(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "\n",
    "def remove_mentions(sentence):\n",
    "    # keep the @ to check for mentions among separate groups\n",
    "    return re.sub(r'@#?\\b\\w\\w+\\b', '@', sentence)\n",
    "\n",
    "def valid_token(tok):\n",
    "    if '#' in tok:\n",
    "        # make sure the hashtag is alphanumeric (avoiding arabic etc)\n",
    "        nohash = tok[1:]\n",
    "        is_latin = re.sub('[^0-9a-zA-Z]+', '', nohash) == nohash\n",
    "        if is_latin:\n",
    "            return tok\n",
    "        else:\n",
    "            return ''\n",
    "    non_stop = tok not in stop_words\n",
    "    no_rt = 'rt' not in tok\n",
    "    is_latin = re.sub('[^0-9a-zA-Z]+', '', tok) == tok\n",
    "    return is_latin and non_stop\n",
    "\n",
    "def clean_stopwords(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    return ' '.join([t for t in tokens if valid_token(t)])\n",
    "        \n",
    "def stem(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return ' '.join([t for t in tokens if valid_token(t)])\n",
    "\n",
    "def empty_to_nan(sentence):\n",
    "    if len(sentence) < 1:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return sentence\n",
    "\n",
    "def clean_all(s):\n",
    "    #s = s.lower()\n",
    "    s = remove_links_and_html(s)\n",
    "    s = remove_punct(s)\n",
    "    s = remove_mentions(s)\n",
    "    s = clean_stopwords(s)\n",
    "    # stemming is slow on loads of data, consider uncommenting on big sets.\n",
    "    #s = stem(s)\n",
    "    # finally, make sure we have no empty texts\n",
    "    s = empty_to_nan(s)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = glob.glob(os.path.join(os.getcwd(), 'SPLIT') + '/*')\n",
    "labels = pd.read_json('../data/labels.ndjson', lines=True)\n",
    "for sample in samples:\n",
    "    csvname = 'SPLIT/' + sample[-6:] + '.csv'\n",
    "    print(csvname)\n",
    "    print(sample)\n",
    "    df = pd.read_json(sample, lines=True)\n",
    "    combined = pd.merge(df, labels, on='id')\n",
    "    #combined = combined.drop(columns=['id'])  # no need for any non-label data\n",
    "    combined['text'] = combined['text'].apply(merge_sents)\n",
    "    combined = combined.dropna()\n",
    "    print(combined.head())\n",
    "    del df\n",
    "    \n",
    "    start = time.time()\n",
    "    print('...cleaning')\n",
    "    combined['text'] = combined['text'].str.lower()\n",
    "    combined['text'] = combined['text'].apply(clean_all)\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "\n",
    "    # prune empty texts\n",
    "    combined = combined.dropna()\n",
    "    combined.to_csv(csvname, header=False, index=False)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
