{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = 'combined.csv'\n",
    "test_csv = 'combined-test.csv'\n",
    "\n",
    "train_path = 'samples'\n",
    "test_path = 'samples-test'\n",
    "\n",
    "# modify this shit if you want to train :)\n",
    "test = True\n",
    "\n",
    "if test:\n",
    "    _csv = test_csv\n",
    "    _path = test_path\n",
    "else:\n",
    "    _csv = train_csv\n",
    "    _path = train_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = None\n",
    "if not os.path.isfile(_csv):\n",
    "    samples = glob.glob(os.path.join(os.getcwd(), _path) + '/*')\n",
    "\n",
    "    dfs = []\n",
    "    for sample in samples:\n",
    "        print(sample)\n",
    "        df = pd.read_json(sample, lines=True)\n",
    "        dfs.append(df)\n",
    "\n",
    "    final_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "    labels = pd.read_json('../data/labels.ndjson', lines=True)\n",
    "    combined = pd.merge(final_df, labels, on='id')\n",
    "    \n",
    "    def merge_sents(sent):\n",
    "        return '. '.join(sent)\n",
    "    \n",
    "    combined['text'] = combined['text'].apply(merge_sents)\n",
    "    combined.to_csv(_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500, 5)\n",
      "(3500, 5)\n"
     ]
    }
   ],
   "source": [
    "if not final_df:  # make sure we didn't write to a csv\n",
    "    data_path = os.path.join(os.getcwd(), _csv)\n",
    "    data_raw = pd.read_csv(data_path)\n",
    "    #data_raw = pd.read_csv(open(data_path,'r'), encoding='utf-8', engine='c')\n",
    "    data_raw.columns = ['index', 'id', 'text', 'birthyear', 'fame', 'gender', 'occupation']\n",
    "    data_raw = data_raw.drop(columns=['index', 'id'])  # no need for any non-label data\n",
    "\n",
    "    print(data_raw.shape)  # before dropping NaN values\n",
    "\n",
    "    data_raw = data_raw.dropna()\n",
    "    #data_raw['birthyear'] = data_raw['birthyear'].astype(int)  # from 1978.0 -> 1978\n",
    "    print(data_raw.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.head()\n",
    "data = data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess!\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['rt'])  # remove the retweet tag!\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "import re\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links_and_html(sentence):\n",
    "    sentence = re.sub(r'http\\S+', '', sentence)\n",
    "    sentence = re.sub(r'<[^<]+?>', '', sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def remove_punct(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "\n",
    "def remove_mentions(sentence):\n",
    "    # keep the @ to check for mentions among separate groups\n",
    "    return re.sub(r'@#?\\b\\w\\w+\\b', '@', sentence)\n",
    "\n",
    "def valid_token(tok):\n",
    "    if '#' in tok:\n",
    "        # make sure the hashtag is alphanumeric (avoiding arabic etc)\n",
    "        return re.sub('[^0-9a-zA-Z]+', '', tok) != ''\n",
    "    non_stop = tok not in stop_words\n",
    "    no_rt = 'rt' not in tok\n",
    "    is_latin = re.sub('[^0-9a-zA-Z]+', '', tok) == tok\n",
    "    return is_latin and non_stop\n",
    "\n",
    "def clean_stopwords(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    return ' '.join([t for t in tokens if valid_token(t)])\n",
    "        \n",
    "def stem(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return ' '.join([t for t in tokens if valid_token(t)])\n",
    "\n",
    "def empty_to_nan(sentence):\n",
    "    if len(sentence) < 1:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return sentence\n",
    "\n",
    "def clean_all(s):\n",
    "    #s = s.lower()\n",
    "    s = remove_links_and_html(s)\n",
    "    s = remove_punct(s)\n",
    "    s = remove_mentions(s)\n",
    "    s = clean_stopwords(s)\n",
    "    # stemming is slow on loads of data, consider uncommenting on big sets.\n",
    "    #s = stem(s)\n",
    "    # finally, make sure we have no empty texts\n",
    "    s = empty_to_nan(s)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211.90762305259705\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>birthyear</th>\n",
       "      <th>fame</th>\n",
       "      <th>gender</th>\n",
       "      <th>occupation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>awwww thank much best year equipped ive advent...</td>\n",
       "      <td>1989</td>\n",
       "      <td>superstar</td>\n",
       "      <td>female</td>\n",
       "      <td>performer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tell secrets dog never heard dog talk beloved ...</td>\n",
       "      <td>1988</td>\n",
       "      <td>superstar</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>backatcha thanks known gonna live long taken b...</td>\n",
       "      <td>1958</td>\n",
       "      <td>superstar</td>\n",
       "      <td>male</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>laugh loud friends never seen gets every time ...</td>\n",
       "      <td>1992</td>\n",
       "      <td>superstar</td>\n",
       "      <td>female</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heartbreaking #ripmacmiller hours still answer...</td>\n",
       "      <td>1994</td>\n",
       "      <td>superstar</td>\n",
       "      <td>female</td>\n",
       "      <td>performer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yes last night #nkotbcruisex #repost yesterday...</td>\n",
       "      <td>1976</td>\n",
       "      <td>superstar</td>\n",
       "      <td>female</td>\n",
       "      <td>performer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>happy world straight edge day given day usuall...</td>\n",
       "      <td>1989</td>\n",
       "      <td>superstar</td>\n",
       "      <td>female</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hard think week ago strolling streets palma he...</td>\n",
       "      <td>1980</td>\n",
       "      <td>superstar</td>\n",
       "      <td>female</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gazetecilik tam da budur gazeteci diye dalkavu...</td>\n",
       "      <td>1972</td>\n",
       "      <td>superstar</td>\n",
       "      <td>male</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>made proud bruins 2019 schedule mark calendars...</td>\n",
       "      <td>1996</td>\n",
       "      <td>superstar</td>\n",
       "      <td>female</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>brother forever grateful #aaronspelling bringi...</td>\n",
       "      <td>1974</td>\n",
       "      <td>superstar</td>\n",
       "      <td>female</td>\n",
       "      <td>performer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hot puerto ricans americans oh vote #novemberi...</td>\n",
       "      <td>1984</td>\n",
       "      <td>superstar</td>\n",
       "      <td>female</td>\n",
       "      <td>performer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>im proud iya glory #nighttimeshowib w x overdo...</td>\n",
       "      <td>2000</td>\n",
       "      <td>superstar</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>emailed julie got kicked back also dmed dont w...</td>\n",
       "      <td>1985</td>\n",
       "      <td>superstar</td>\n",
       "      <td>female</td>\n",
       "      <td>creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>los angeles literally take car away dont drive...</td>\n",
       "      <td>1986</td>\n",
       "      <td>superstar</td>\n",
       "      <td>male</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>continue surprised glass game love squad rocks...</td>\n",
       "      <td>1971</td>\n",
       "      <td>superstar</td>\n",
       "      <td>female</td>\n",
       "      <td>performer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>last half second grabs bottom back best seen w...</td>\n",
       "      <td>1989</td>\n",
       "      <td>superstar</td>\n",
       "      <td>male</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rest peace coach many many prayers family grea...</td>\n",
       "      <td>1995</td>\n",
       "      <td>superstar</td>\n",
       "      <td>male</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>yes initially felt trying change behavior valu...</td>\n",
       "      <td>1980</td>\n",
       "      <td>superstar</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>find link latest issue newsletter online gotta...</td>\n",
       "      <td>1971</td>\n",
       "      <td>superstar</td>\n",
       "      <td>female</td>\n",
       "      <td>creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>thanks wishes lots love #ppdkp thanks dhanywad...</td>\n",
       "      <td>1956</td>\n",
       "      <td>superstar</td>\n",
       "      <td>male</td>\n",
       "      <td>creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>liked tweet last night accident apologies prob...</td>\n",
       "      <td>1990</td>\n",
       "      <td>superstar</td>\n",
       "      <td>male</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>weird alarmed left son nanny watching ny giant...</td>\n",
       "      <td>1970</td>\n",
       "      <td>superstar</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ok already found someone replace done done bes...</td>\n",
       "      <td>1984</td>\n",
       "      <td>superstar</td>\n",
       "      <td>female</td>\n",
       "      <td>performer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>note release day long rant ultimate truth matt...</td>\n",
       "      <td>1975</td>\n",
       "      <td>superstar</td>\n",
       "      <td>male</td>\n",
       "      <td>creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>download first episode #mishaps #podcast thank...</td>\n",
       "      <td>1981</td>\n",
       "      <td>superstar</td>\n",
       "      <td>female</td>\n",
       "      <td>performer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>skating back #kinsleefirsttime friends working...</td>\n",
       "      <td>1986</td>\n",
       "      <td>superstar</td>\n",
       "      <td>male</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>dreamforce show sf weeks back #halloween seaso...</td>\n",
       "      <td>1962</td>\n",
       "      <td>superstar</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>wood grain love life clown clown2076s photo #l...</td>\n",
       "      <td>1982</td>\n",
       "      <td>superstar</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>met danny leiner shorts played together aspen ...</td>\n",
       "      <td>1969</td>\n",
       "      <td>superstar</td>\n",
       "      <td>male</td>\n",
       "      <td>creator</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  birthyear       fame  \\\n",
       "0   awwww thank much best year equipped ive advent...       1989  superstar   \n",
       "1   tell secrets dog never heard dog talk beloved ...       1988  superstar   \n",
       "2   backatcha thanks known gonna live long taken b...       1958  superstar   \n",
       "3   laugh loud friends never seen gets every time ...       1992  superstar   \n",
       "4   heartbreaking #ripmacmiller hours still answer...       1994  superstar   \n",
       "5   yes last night #nkotbcruisex #repost yesterday...       1976  superstar   \n",
       "6   happy world straight edge day given day usuall...       1989  superstar   \n",
       "7   hard think week ago strolling streets palma he...       1980  superstar   \n",
       "8   gazetecilik tam da budur gazeteci diye dalkavu...       1972  superstar   \n",
       "9   made proud bruins 2019 schedule mark calendars...       1996  superstar   \n",
       "10  brother forever grateful #aaronspelling bringi...       1974  superstar   \n",
       "11  hot puerto ricans americans oh vote #novemberi...       1984  superstar   \n",
       "12  im proud iya glory #nighttimeshowib w x overdo...       2000  superstar   \n",
       "13  emailed julie got kicked back also dmed dont w...       1985  superstar   \n",
       "14  los angeles literally take car away dont drive...       1986  superstar   \n",
       "15  continue surprised glass game love squad rocks...       1971  superstar   \n",
       "16  last half second grabs bottom back best seen w...       1989  superstar   \n",
       "17  rest peace coach many many prayers family grea...       1995  superstar   \n",
       "18  yes initially felt trying change behavior valu...       1980  superstar   \n",
       "19  find link latest issue newsletter online gotta...       1971  superstar   \n",
       "20  thanks wishes lots love #ppdkp thanks dhanywad...       1956  superstar   \n",
       "21  liked tweet last night accident apologies prob...       1990  superstar   \n",
       "22  weird alarmed left son nanny watching ny giant...       1970  superstar   \n",
       "23  ok already found someone replace done done bes...       1984  superstar   \n",
       "24  note release day long rant ultimate truth matt...       1975  superstar   \n",
       "25  download first episode #mishaps #podcast thank...       1981  superstar   \n",
       "26  skating back #kinsleefirsttime friends working...       1986  superstar   \n",
       "27  dreamforce show sf weeks back #halloween seaso...       1962  superstar   \n",
       "28  wood grain love life clown clown2076s photo #l...       1982  superstar   \n",
       "29  met danny leiner shorts played together aspen ...       1969  superstar   \n",
       "\n",
       "    gender occupation  \n",
       "0   female  performer  \n",
       "1     male  performer  \n",
       "2     male     sports  \n",
       "3   female     sports  \n",
       "4   female  performer  \n",
       "5   female  performer  \n",
       "6   female     sports  \n",
       "7   female     sports  \n",
       "8     male    science  \n",
       "9   female     sports  \n",
       "10  female  performer  \n",
       "11  female  performer  \n",
       "12    male  performer  \n",
       "13  female    creator  \n",
       "14    male     sports  \n",
       "15  female  performer  \n",
       "16    male     sports  \n",
       "17    male     sports  \n",
       "18    male  performer  \n",
       "19  female    creator  \n",
       "20    male    creator  \n",
       "21    male     sports  \n",
       "22    male  performer  \n",
       "23  female  performer  \n",
       "24    male    creator  \n",
       "25  female  performer  \n",
       "26    male     sports  \n",
       "27    male  performer  \n",
       "28    male  performer  \n",
       "29    male    creator  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "data['text'] = data['text'].str.lower()\n",
    "data['text'] = data['text'].apply(clean_all)\n",
    "# run time: around 3-4 minutes per 1 million texts\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "# prune empty texts\n",
    "data = data.dropna()\n",
    "data.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3499, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_csv = 'cleaned-' + _csv\n",
    "data.to_csv(cleaned_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def mkdir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    else:\n",
    "        print('folder exists: {}'.format(path))\n",
    "\n",
    "# the folder to hold the datasets (csv) with [text, label]\n",
    "single_label_dir = os.path.join(os.getcwd(), 'single-labeled-combined-text')\n",
    "mkdir(single_label_dir)\n",
    "\n",
    "categories = list(data.columns.values)\n",
    "categories = categories[1:]\n",
    "print(categories)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for categ in categories:\n",
    "    print(categ)\n",
    "    vals = sorted(data[categ].unique())\n",
    "    print(vals)\n",
    "    # create a folder for each category\n",
    "    categ_path = os.path.join(single_label_dir, categ)\n",
    "    mkdir(categ_path)\n",
    "    # store each corresponding dataframe in respective category folders        \n",
    "    for val in vals:\n",
    "        #print('{}: {}'.format(val, type(val)))\n",
    "        condition = data[categ] == val\n",
    "        tmp_df = data[condition][['text', categ]]  # this extracts the text and column\n",
    "        save_path = categ_path + '\\\\' + str(val) + '.csv'\n",
    "        print('storing {}'.format(save_path))\n",
    "        tmp_df.to_csv(save_path)\n",
    "        del tmp_df\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
