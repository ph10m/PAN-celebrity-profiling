{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split as SPLIT\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=1, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=10, batch_size=2, verbose=0)\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(32,32,32), n_channels=1, n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        #indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        #list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        #X, y = self.__data_generation(list_IDs_temp)\n",
    "        X, y = self.list_IDs, self.labels\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            #X[i,] = np.load('data/' + ID + '.npy')\n",
    "            X[i,] = list_IDs\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "def get_params(n_classes):\n",
    "    return {'dim': (32,32,32),\n",
    "          'batch_size': 64,\n",
    "          'n_classes': n_classes,  # the num of unique classes for a Y-set\n",
    "          'n_channels': 1,\n",
    "          'shuffle': True}\n",
    "\n",
    "# Datasets\n",
    "#partition = # IDs\n",
    "#labels = # Labels\n",
    "\n",
    "# Generators\n",
    "def get_generator(df, labels, params):\n",
    "    #training_generator = DataGenerator(partition['train'], labels, **params)\n",
    "    return DataGenerator(df, labels, **params)\n",
    "\n",
    "#validation_generator = DataGenerator(partition['validation'], labels, **params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dic(d):\n",
    "    count = 0\n",
    "    for k,v in d.items():\n",
    "        if count > 20:\n",
    "            return\n",
    "        count += 1\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['birthyear', 'birthyear.csv', 'birthyearOG.csv', 'fame', 'gender', 'occupation']\n",
      "birthyear\n",
      "C:\\Users\\Tollef\\Desktop\\Spring2019\\TextAnalysis\\project\\PAN-celebrity-profiling\\data\\single-label\\birthyear.csv\n",
      "['1940.csv', '1941.csv', '1942.csv', '1943.csv', '1944.csv', '1945.csv', '1946.csv', '1947.csv', '1948.csv', '1949.csv', '1950.csv', '1951.csv', '1952.csv', '1953.csv', '1954.csv', '1955.csv', '1956.csv', '1957.csv', '1958.csv', '1959.csv', '1960.csv', '1961.csv', '1962.csv', '1963.csv', '1964.csv', '1965.csv', '1966.csv', '1967.csv', '1968.csv', '1969.csv', '1970.csv', '1971.csv', '1972.csv', '1973.csv', '1974.csv', '1975.csv', '1976.csv', '1977.csv', '1978.csv', '1979.csv', '1980.csv', '1981.csv', '1982.csv', '1983.csv', '1984.csv', '1985.csv', '1986.csv', '1987.csv', '1988.csv', '1989.csv', '1990.csv', '1991.csv', '1992.csv', '1993.csv', '1994.csv', '1995.csv', '1996.csv', '1997.csv', '1998.csv', '1999.csv', '2000.csv', '2001.csv', '2002.csv', '2003.csv', '2004.csv', '2005.csv', '2007.csv', '2008.csv']\n",
      "   Unnamed: 0  Unnamed: 0.1  \\\n",
      "0           0       1260863   \n",
      "1           1       1260864   \n",
      "2           2       1260865   \n",
      "3           3       1260866   \n",
      "4           4       1260867   \n",
      "\n",
      "                                                text  birthyear  \n",
      "0  started reading new autobiography romance anec...       1940  \n",
      "1  meet inspirational 2018 #nansenaward winner dr...       1940  \n",
      "2  260 million children couldnt go school morning...       1940  \n",
      "3  journey begun kirsten beyer michael chabon aki...       1940  \n",
      "4  last week rescued 45 dogs suspected dog fighti...       1940  \n",
      "(499999, 4)\n",
      "        Unnamed: 0  Unnamed: 0.1  \\\n",
      "295298      295298       7160686   \n",
      "51643        51643       9532133   \n",
      "19687        19687       5086859   \n",
      "373082      373082       8788427   \n",
      "272795      272795       3652663   \n",
      "\n",
      "                                                     text  birthyear  \n",
      "295298  imagine sheer terror panic farm animals face t...       1946  \n",
      "51643   forecast european model exceptional error less...       1941  \n",
      "19687                           round qualifying underway       1940  \n",
      "373082                               obviously good taste       1947  \n",
      "272795  continent ravaged war unique union strong comm...       1946  \n",
      "        Unnamed: 0  Unnamed: 0.1  \\\n",
      "51993        51993       9532485   \n",
      "147245      147245       8877243   \n",
      "159444      159444      12696870   \n",
      "32338        32338      12983879   \n",
      "189639      189639       6052698   \n",
      "\n",
      "                                                     text  birthyear  \n",
      "51993                                              say hi       1941  \n",
      "147245  mean also theres another guy watch thats right...       1943  \n",
      "159444  environment partisan issue science denial cost...       1943  \n",
      "32338   poll shows decline support latest #iranians vi...       1940  \n",
      "189639  today day accountability govt give report card...       1944  \n",
      "imagine sheer terror panic farm animals face trucks bound slaughterhouse please 1946\n",
      "forecast european model exceptional error less 100 miles compare models 1941\n",
      "round qualifying underway 1940\n",
      "obviously good taste 1947\n",
      "continent ravaged war unique union strong common values europe eu achieved something 1946\n",
      "charles 1948\n",
      "set release songs vault tomorrow new album #24karatgold 1948\n",
      "strong #armycombine continues 2019 ot bryan hudson says #vatech #lsu currently recruiting 1945\n",
      "absolutely gutted amazing support always #grandfinal 1948\n",
      "talking enemies teenage mutant ninja turtles feedback getting driver regards car handling #nascarplayoffs 1940\n",
      "tonight #nomoretours2 ft lauderdale jacksonville fl friday 1948\n",
      "stockton u say corruption charges extremely important significant factor vote hugin gets gop menendez dems independents supporting hugin margin 1945\n",
      "nice quote sure burns 1941\n",
      "want tell sir books beacon rational light catholic spiritual 1941\n",
      "menerima karikatur perdamaian dari universitas islam malang #kunkerwapres 1942\n",
      "unfortunately youre right sickening 1946\n",
      "albury resources make better fist wagga tigers decade ago #honesty 1941\n",
      "americans hungry democracy heres great since 2017 weve almost doubled number states using automatic voter registration 13 less hassle cheaper voters thx oregon 15 showing way 1944\n",
      "poor corrupted soul 1945\n",
      "skin look radiant keep shining love 1941\n",
      "yep good player 1941\n",
      "say hi 1941\n",
      "mean also theres another guy watch thats right im talking treadwell 1943\n",
      "environment partisan issue science denial costing us time resources #savethebay 1943\n",
      "poll shows decline support latest #iranians view changing 1940\n",
      "today day accountability govt give report card also day accountability 1944\n",
      "local antifa reccruiting office looking cowards like 1942\n",
      "1 5 medical faculty got none comprehension calculation questions right statistical literacy 1943\n",
      "behind scenes footage 1948\n",
      "well crafted flashes rage family loss nikolas lifetime trouble via 1942\n",
      "ill thurs 8 16 pm firestone tire 1600 regent ave west stop 1940\n",
      "true faith obedient faith disobedient faith unbelief 1945\n",
      "foi request hse expenditure black ink could prove interesting 1944\n",
      "u denying passports americans along border throwing citizenship #americans #citizenship #border 866 594 hope #iheartradio: 1941\n",
      "one missing 1948\n",
      "realized song way ahead time 1940\n",
      "aww thanks 1943\n",
      "tune discuss illogical deeply worrying decision leave #parisagreement: 1940\n",
      "makes amateur golfers feel little bit better 1948\n",
      "man 1940\n",
      "fans uk premiere captainamerica ig supporting charges 1948\n",
      "uh deprive minority americans affirmative action opportunities lying #hypocrite 1947\n",
      "6 unique labels\n",
      "{'dim': (32, 32, 32), 'batch_size': 64, 'n_classes': 6, 'n_channels': 1, 'shuffle': True}\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No data provided for \"dense_20\". Need data for each key in: ['dense_20']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'DataFrame'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m             ]\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'DataFrame'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m             ]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'dense_20'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-4b1c8274ddd7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mtrain_gen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mtest_gen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1209\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1210\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1211\u001b[1;33m             class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1212\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m     76\u001b[0m             raise ValueError('No data provided for \"' + e.args[0] +\n\u001b[0;32m     77\u001b[0m                              \u001b[1;34m'\". Need data '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                              'for each key in: ' + str(names))\n\u001b[0m\u001b[0;32m     79\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No data provided for \"dense_20\". Need data for each key in: ['dense_20']"
     ]
    }
   ],
   "source": [
    "# the folder to hold the datasets (csv) with [text, label]\n",
    "single_label_dir = os.path.join(os.getcwd(), 'single-label')\n",
    "\n",
    "categories = os.listdir(single_label_dir)\n",
    "print(categories)\n",
    "unique_labels = len(categories)\n",
    "\n",
    "for categ in categories:\n",
    "    print(categ)\n",
    "    categ_path = os.path.join(single_label_dir, categ)\n",
    "    categ_save_path = os.path.join(single_label_dir, categ) + '.csv'\n",
    "    print(categ_save_path)\n",
    "    \n",
    "    vals = os.listdir(categ_path)\n",
    "    print(vals)\n",
    "    df = None\n",
    "    if not os.path.isfile(categ_save_path):\n",
    "        categ_frames =  []\n",
    "        for val in vals:\n",
    "            dataset = categ_path + '\\\\' + str(val)\n",
    "            categ_frames.append(pd.read_csv(dataset))\n",
    "        df = pd.concat(categ_frames, axis=0, ignore_index=True)\n",
    "        df.to_csv(categ_save_path)\n",
    "    else:\n",
    "        df = pd.read_csv(categ_save_path)\n",
    "        \n",
    "    # GENERATOR COMPUTATION OF EACH MAJOR CSV!\n",
    "    print(df.head())\n",
    "    print(df.shape)\n",
    "    \n",
    "    # create a train/validation set of the text-column\n",
    "    \n",
    "    train, validation = SPLIT(df, test_size=0.1)\n",
    "    print(train.head())\n",
    "    print(validation.head())\n",
    "    train_text = validation.text\n",
    "    #print(train_text[:20])\n",
    "    validation_text = validation.text\n",
    "    #print(validation_text[:10])\n",
    "    \n",
    "    # labels is a dict of \"TEXT\"->\"LABEL\" (key->val)\n",
    "    KEY = 'text'\n",
    "    train_labels = train.set_index(KEY)[categ].to_dict()\n",
    "    dic(train_labels)\n",
    "    validation_labels = validation.set_index(KEY)[categ].to_dict()\n",
    "    dic(validation_labels)\n",
    "    \n",
    "    # Design model - new for each category\n",
    "    model = baseline_model()\n",
    "\n",
    "    # Train model on dataset\n",
    "    print('{} unique labels'.format(unique_labels))\n",
    "    params = get_params(unique_labels)\n",
    "    print(params)\n",
    "    train_gen = get_generator(train_text, train_labels, params)\n",
    "    test_gen = get_generator(validation_text, validation_labels, params)\n",
    "    model.fit_generator(generator=train_gen, validation_data=test_gen)\n",
    "\n",
    "    del df\n",
    "    del X_train\n",
    "    del X_test\n",
    "    del y_train\n",
    "    del y_test\n",
    "    #le = LabelEncoder()\n",
    "    #le.fit(_Y)\n",
    "    #le_Y = le.transform(_Y)\n",
    "    #Y_hot = np_utils.to_categorical(le_Y)\n",
    "    #results = cross_val_score(estimator, _X, Y_hot, cv=kfold)\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
