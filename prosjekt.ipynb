{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "#printmd('**bold**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file exists, continue!\n"
     ]
    }
   ],
   "source": [
    "# skip this field if \"combined_set.csv\" exists\n",
    "parsed_csv = 'combined_set.csv'\n",
    "\n",
    "if not os.path.isfile(parsed_csv):\n",
    "    feeds = pd.read_json('./feeds_2000.ndjson', lines=True)\n",
    "    labels = pd.read_json('./labels_2000.ndjson', lines=True)\n",
    "\n",
    "    # not all IDs are found in the tiny labels file, make sure we have a complete (albeit fake, dataset)\n",
    "    import random\n",
    "    valid_ids = list(feeds.id)  # the ids found in the feeds_2000\n",
    "    random.shuffle(valid_ids)  # shuffle these and assign random IDs that exist \n",
    "    labels.id = valid_ids\n",
    "    combined = pd.merge(feeds, labels, on='id')\n",
    "    expanded = combined.set_index(\n",
    "        ['id', 'birthyear', 'fame', 'gender', 'occupation']\n",
    "    )['text'].apply(pd.Series).stack()\n",
    "    expanded = expanded.reset_index()\n",
    "    expanded = expanded.drop(columns=['level_5'])  # level_5 is the auto-generated new column, containing an index\n",
    "\n",
    "    expanded.to_csv(parsed_csv)\n",
    "else:\n",
    "    print('file exists, continue!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getcwd(), parsed_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5220005, 5)\n",
      "(5219253, 5)\n"
     ]
    }
   ],
   "source": [
    "#data_raw = pd.read_csv(data_path)\n",
    "data_raw = pd.read_csv(open(data_path,'r'), encoding='utf-8', engine='c')\n",
    "data_raw.columns = ['index', 'id', 'birthyear', 'fame', 'gender', 'occupation', 'text']\n",
    "data_raw = data_raw.drop(columns=['index', 'id'])  # no need for any non-label data\n",
    "\n",
    "print(data_raw.shape)  # before dropping NaN values\n",
    "\n",
    "data_raw = data_raw.dropna()\n",
    "data_raw['birthyear'] = data_raw['birthyear'].astype(int)  # from 1978.0 -> 1978\n",
    "print(data_raw.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in data = 5219253\n",
      "Number of columns in data = 5\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Sample data:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>birthyear</th>\n",
       "      <th>fame</th>\n",
       "      <th>gender</th>\n",
       "      <th>occupation</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>Back at it with @americanidol looking for...he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>Can‚Äôt buy all the happiness in the world, it‚Äôs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>30 down @nytimes ü§ùüß°</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>üì∏ @ronyalwin üíò @ New York, New York https://t....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>üéÄ pink it was love at first sight üéÄ @ New York...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>Putting my best foot forward in The Memphis by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>Girls UNITED can never be divided! üëØ‚Äç‚ôÄÔ∏è‚ù§Ô∏è http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>BRB buying The Stephanie bow shoe as an early ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>Connect the dots, bbs  #TheClara - TeamKP @kpc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>Geometry was The Daina‚Äôs favorite subject in s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>We love a Mary June. I mean, Mary Jane. I mean...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>WE HAVE ALWAYS BEEN READY FOR THIS GELI OK üçë üçã...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>Ain‚Äôt The Turner a jewel?  üíé - TeamKP @kpcolle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>KICK üë£ BALL üë£ CHANGE in The Jo - TeamKP @kpcol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>All I want for Christmas is The Caine (sorry, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>We love The Sissy as much as we love our own s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>Live in FIVE!  -TeamKP @kpcollections #KatyPer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>T-minus one hour until I‚Äôll be on @QVC showing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>Tune in live now for a chat with @QVC! https:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>WHAT SHE SAID OK\\n    ‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è\\n\\n#Repost @r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>Incredibly moving and authentic ‚ù§Ô∏èCongrats fri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>PROUD to be covergirl for @FootwearNews &amp; even...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>I like when a lone fly catches a ride on an in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>Why didn‚Äôt the fettuccine go out for Halloween...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>Ladies, it‚Äôs time to remind them ‚úäüèª</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>RT @CoryBooker: Right, forever vigilant \\n\\nIs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>#TBT to when I got to ‚úî another one off my buc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>Proud of this üë†PLATFORM üë† (yes I did!) to rais...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>*BLOCKS* https://t.co/ntOG1A7ede</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>I just cracked my molar in half on a ranch cor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    birthyear  fame gender occupation  \\\n",
       "0        1991  star   male  performer   \n",
       "1        1991  star   male  performer   \n",
       "2        1991  star   male  performer   \n",
       "3        1991  star   male  performer   \n",
       "4        1991  star   male  performer   \n",
       "5        1991  star   male  performer   \n",
       "6        1991  star   male  performer   \n",
       "7        1991  star   male  performer   \n",
       "8        1991  star   male  performer   \n",
       "9        1991  star   male  performer   \n",
       "10       1991  star   male  performer   \n",
       "11       1991  star   male  performer   \n",
       "12       1991  star   male  performer   \n",
       "13       1991  star   male  performer   \n",
       "14       1991  star   male  performer   \n",
       "15       1991  star   male  performer   \n",
       "16       1991  star   male  performer   \n",
       "17       1991  star   male  performer   \n",
       "18       1991  star   male  performer   \n",
       "19       1991  star   male  performer   \n",
       "20       1991  star   male  performer   \n",
       "21       1991  star   male  performer   \n",
       "22       1991  star   male  performer   \n",
       "23       1991  star   male  performer   \n",
       "24       1991  star   male  performer   \n",
       "25       1991  star   male  performer   \n",
       "26       1991  star   male  performer   \n",
       "27       1991  star   male  performer   \n",
       "28       1991  star   male  performer   \n",
       "29       1991  star   male  performer   \n",
       "\n",
       "                                                 text  \n",
       "0   Back at it with @americanidol looking for...he...  \n",
       "1   Can‚Äôt buy all the happiness in the world, it‚Äôs...  \n",
       "2                                 30 down @nytimes ü§ùüß°  \n",
       "3   üì∏ @ronyalwin üíò @ New York, New York https://t....  \n",
       "4   üéÄ pink it was love at first sight üéÄ @ New York...  \n",
       "5   Putting my best foot forward in The Memphis by...  \n",
       "6   Girls UNITED can never be divided! üëØ‚Äç‚ôÄÔ∏è‚ù§Ô∏è http...  \n",
       "7   BRB buying The Stephanie bow shoe as an early ...  \n",
       "8   Connect the dots, bbs  #TheClara - TeamKP @kpc...  \n",
       "9   Geometry was The Daina‚Äôs favorite subject in s...  \n",
       "10  We love a Mary June. I mean, Mary Jane. I mean...  \n",
       "11  WE HAVE ALWAYS BEEN READY FOR THIS GELI OK üçë üçã...  \n",
       "12  Ain‚Äôt The Turner a jewel?  üíé - TeamKP @kpcolle...  \n",
       "13  KICK üë£ BALL üë£ CHANGE in The Jo - TeamKP @kpcol...  \n",
       "14  All I want for Christmas is The Caine (sorry, ...  \n",
       "15  We love The Sissy as much as we love our own s...  \n",
       "16  Live in FIVE!  -TeamKP @kpcollections #KatyPer...  \n",
       "17  T-minus one hour until I‚Äôll be on @QVC showing...  \n",
       "18  Tune in live now for a chat with @QVC! https:/...  \n",
       "19  WHAT SHE SAID OK\\n    ‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è\\n\\n#Repost @r...  \n",
       "20  Incredibly moving and authentic ‚ù§Ô∏èCongrats fri...  \n",
       "21  PROUD to be covergirl for @FootwearNews & even...  \n",
       "22  I like when a lone fly catches a ride on an in...  \n",
       "23  Why didn‚Äôt the fettuccine go out for Halloween...  \n",
       "24                Ladies, it‚Äôs time to remind them ‚úäüèª  \n",
       "25  RT @CoryBooker: Right, forever vigilant \\n\\nIs...  \n",
       "26  #TBT to when I got to ‚úî another one off my buc...  \n",
       "27  Proud of this üë†PLATFORM üë† (yes I did!) to rais...  \n",
       "28                   *BLOCKS* https://t.co/ntOG1A7ede  \n",
       "29  I just cracked my molar in half on a ranch cor...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of rows in data =\",data_raw.shape[0])\n",
    "print(\"Number of columns in data =\",data_raw.shape[1])\n",
    "print(\"\\n\")\n",
    "printmd(\"**Sample data:**\")\n",
    "data_raw.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['birthyear', 'fame', 'gender', 'occupation']\n"
     ]
    }
   ],
   "source": [
    "categories = list(data_raw.columns.values)\n",
    "categories = categories[:-1]\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['rt'])  # remove the retweet tag!\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "import re\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500000, 5)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data_raw\n",
    "# uncomment below to keep a subset of rows for testing\n",
    "numrows = 500000\n",
    "data = data_raw.loc[np.random.choice(data_raw.index, size=numrows)]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links_and_html(sentence):\n",
    "    sentence = re.sub(r'http\\S+', '', sentence)\n",
    "    sentence = re.sub(r'<[^<]+?>', '', sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def remove_punct(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "\n",
    "def remove_mentions(sentence):\n",
    "    return re.sub(r'@#?\\b\\w\\w+\\b', '', sentence)\n",
    "\n",
    "def valid_token(tok):\n",
    "    if '#' in tok:\n",
    "        # make sure the hashtag is alphanumeric (avoiding arabic etc)\n",
    "        return re.sub('[^0-9a-zA-Z]+', '', tok) != ''\n",
    "    non_stop = tok not in stop_words\n",
    "    no_rt = 'rt' not in tok\n",
    "    is_latin = re.sub('[^0-9a-zA-Z]+', '', tok) == tok\n",
    "    return is_latin and non_stop\n",
    "\n",
    "def clean_stopwords(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    return ' '.join([t for t in tokens if valid_token(t)])\n",
    "        \n",
    "def stem(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return ' '.join([t for t in tokens if valid_token(t)])\n",
    "\n",
    "def empty_to_nan(sentence):\n",
    "    if len(sentence) < 1:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return sentence\n",
    "\n",
    "def clean_all(s):\n",
    "    #s = s.lower()\n",
    "    s = remove_links_and_html(s)\n",
    "    s = remove_punct(s)\n",
    "    s = remove_mentions(s)\n",
    "    s = clean_stopwords(s)\n",
    "    # stemming is slow on loads of data, consider uncommenting on big sets.\n",
    "    #s = stem(s)\n",
    "    # finally, make sure we have no empty texts\n",
    "    s = empty_to_nan(s)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.808963775634766\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>birthyear</th>\n",
       "      <th>fame</th>\n",
       "      <th>gender</th>\n",
       "      <th>occupation</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1749932</th>\n",
       "      <td>1994</td>\n",
       "      <td>star</td>\n",
       "      <td>female</td>\n",
       "      <td>sports</td>\n",
       "      <td>cheeky sizzles bop sugar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241123</th>\n",
       "      <td>1992</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>sports</td>\n",
       "      <td>j alvarez haters remix ft bad bunny almighty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5176064</th>\n",
       "      <td>1948</td>\n",
       "      <td>rising</td>\n",
       "      <td>male</td>\n",
       "      <td>science</td>\n",
       "      <td>nyt reporter broke comey story also broke stor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3174102</th>\n",
       "      <td>1987</td>\n",
       "      <td>superstar</td>\n",
       "      <td>male</td>\n",
       "      <td>sports</td>\n",
       "      <td>congrats college graduation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307889</th>\n",
       "      <td>1952</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>#gps ft incluido en disponible en todas las pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497675</th>\n",
       "      <td>1965</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>produced frank dukes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4540824</th>\n",
       "      <td>1986</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>sports</td>\n",
       "      <td>ah got matching purse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4770641</th>\n",
       "      <td>1973</td>\n",
       "      <td>superstar</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>yeah u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811103</th>\n",
       "      <td>1993</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>sports</td>\n",
       "      <td>new years resolutions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2403381</th>\n",
       "      <td>1974</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>professional</td>\n",
       "      <td>#julie2 trailer trending youtube wait see perf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198539</th>\n",
       "      <td>1974</td>\n",
       "      <td>superstar</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>#elmalodelbronx nos vuelve deleitar con su tal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3352593</th>\n",
       "      <td>1970</td>\n",
       "      <td>rising</td>\n",
       "      <td>male</td>\n",
       "      <td>sports</td>\n",
       "      <td>bruce live soho watch live 7pm et</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261602</th>\n",
       "      <td>1984</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>#tbt first time saw octopus together aquarium ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3120906</th>\n",
       "      <td>1983</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>sports</td>\n",
       "      <td>prob hanging w beast teaching showing sweet da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2165393</th>\n",
       "      <td>1960</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>politics</td>\n",
       "      <td>congratulations terrific fight definition true...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4976964</th>\n",
       "      <td>1965</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>politics</td>\n",
       "      <td>tell snuggles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495026</th>\n",
       "      <td>1965</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>thank outpouring affection dad moved beyond wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85014</th>\n",
       "      <td>1980</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>creator</td>\n",
       "      <td>happy birthday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3735477</th>\n",
       "      <td>1953</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>sports</td>\n",
       "      <td>youre ocean soul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4104381</th>\n",
       "      <td>1974</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>semana del que sea muy feliz para todos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505423</th>\n",
       "      <td>1991</td>\n",
       "      <td>superstar</td>\n",
       "      <td>female</td>\n",
       "      <td>performer</td>\n",
       "      <td>page 45 #eotsetlove tweets tom card w loads go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2311087</th>\n",
       "      <td>1983</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>politics</td>\n",
       "      <td>thank #belfastpride host grannie loved every m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3004257</th>\n",
       "      <td>1987</td>\n",
       "      <td>superstar</td>\n",
       "      <td>male</td>\n",
       "      <td>sports</td>\n",
       "      <td>boyz ride</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4098441</th>\n",
       "      <td>1995</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>sports</td>\n",
       "      <td>must visit visit vegas amazing job setting top...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4367958</th>\n",
       "      <td>1941</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>check new single paper cut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3688937</th>\n",
       "      <td>1975</td>\n",
       "      <td>rising</td>\n",
       "      <td>female</td>\n",
       "      <td>performer</td>\n",
       "      <td>u live australia see theosirischild visionary ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4232065</th>\n",
       "      <td>1978</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>creator</td>\n",
       "      <td>wawancara dg radio hamzanwadi 107 fm lombok ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2784689</th>\n",
       "      <td>1983</td>\n",
       "      <td>star</td>\n",
       "      <td>female</td>\n",
       "      <td>sports</td>\n",
       "      <td>ypg pkk #afrinde evlerin afrindeki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5183590</th>\n",
       "      <td>1953</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>sports</td>\n",
       "      <td>thanks inside look dedication commitment #chef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794260</th>\n",
       "      <td>1958</td>\n",
       "      <td>star</td>\n",
       "      <td>female</td>\n",
       "      <td>creator</td>\n",
       "      <td>#justbreathe #cysticfibrosis 14 year old cf he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         birthyear       fame  gender    occupation  \\\n",
       "1749932       1994       star  female        sports   \n",
       "1241123       1992       star    male        sports   \n",
       "5176064       1948     rising    male       science   \n",
       "3174102       1987  superstar    male        sports   \n",
       "1307889       1952       star    male     performer   \n",
       "1497675       1965       star    male     performer   \n",
       "4540824       1986       star    male        sports   \n",
       "4770641       1973  superstar    male     performer   \n",
       "1811103       1993       star    male        sports   \n",
       "2403381       1974       star    male  professional   \n",
       "1198539       1974  superstar    male     performer   \n",
       "3352593       1970     rising    male        sports   \n",
       "261602        1984       star    male     performer   \n",
       "3120906       1983       star    male        sports   \n",
       "2165393       1960       star    male      politics   \n",
       "4976964       1965       star    male      politics   \n",
       "2495026       1965       star    male     performer   \n",
       "85014         1980       star    male       creator   \n",
       "3735477       1953       star    male        sports   \n",
       "4104381       1974       star    male     performer   \n",
       "505423        1991  superstar  female     performer   \n",
       "2311087       1983       star    male      politics   \n",
       "3004257       1987  superstar    male        sports   \n",
       "4098441       1995       star    male        sports   \n",
       "4367958       1941       star    male     performer   \n",
       "3688937       1975     rising  female     performer   \n",
       "4232065       1978       star    male       creator   \n",
       "2784689       1983       star  female        sports   \n",
       "5183590       1953       star    male        sports   \n",
       "1794260       1958       star  female       creator   \n",
       "\n",
       "                                                      text  \n",
       "1749932                           cheeky sizzles bop sugar  \n",
       "1241123       j alvarez haters remix ft bad bunny almighty  \n",
       "5176064  nyt reporter broke comey story also broke stor...  \n",
       "3174102                        congrats college graduation  \n",
       "1307889  #gps ft incluido en disponible en todas las pl...  \n",
       "1497675                               produced frank dukes  \n",
       "4540824                              ah got matching purse  \n",
       "4770641                                             yeah u  \n",
       "1811103                              new years resolutions  \n",
       "2403381  #julie2 trailer trending youtube wait see perf...  \n",
       "1198539  #elmalodelbronx nos vuelve deleitar con su tal...  \n",
       "3352593                  bruce live soho watch live 7pm et  \n",
       "261602   #tbt first time saw octopus together aquarium ...  \n",
       "3120906  prob hanging w beast teaching showing sweet da...  \n",
       "2165393  congratulations terrific fight definition true...  \n",
       "4976964                                      tell snuggles  \n",
       "2495026  thank outpouring affection dad moved beyond wo...  \n",
       "85014                                       happy birthday  \n",
       "3735477                                   youre ocean soul  \n",
       "4104381            semana del que sea muy feliz para todos  \n",
       "505423   page 45 #eotsetlove tweets tom card w loads go...  \n",
       "2311087  thank #belfastpride host grannie loved every m...  \n",
       "3004257                                          boyz ride  \n",
       "4098441  must visit visit vegas amazing job setting top...  \n",
       "4367958                         check new single paper cut  \n",
       "3688937  u live australia see theosirischild visionary ...  \n",
       "4232065  wawancara dg radio hamzanwadi 107 fm lombok ti...  \n",
       "2784689                 ypg pkk #afrinde evlerin afrindeki  \n",
       "5183590  thanks inside look dedication commitment #chef...  \n",
       "1794260  #justbreathe #cysticfibrosis 14 year old cf he...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "data['text'] = data['text'].str.lower()\n",
    "data['text'] = data['text'].apply(clean_all)\n",
    "# run time: around 3-4 minutes per 1 million texts\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "# prune empty texts\n",
    "data = data.dropna()\n",
    "data.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('PREPROCESSED.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('PREPROCESSED.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(372060, 5)\n",
      "(93015, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, random_state=42, test_size=0.20, shuffle=True)\n",
    "\n",
    "#train.to_csv('formatted/train.csv')\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train['text']\n",
    "test_text = test['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents='unicode', sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#x = v.fit_transform(df['Review'].values.astype('U'))  ## Even astype(str) would work\n",
    "vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\n",
    "vectorizer.fit(train_text)\n",
    "vectorizer.fit(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(372060, 3)\n",
      "(93015, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train = vectorizer.transform(train_text)\n",
    "y_train = train.drop(labels = ['text', 'birthyear'], axis=1)\n",
    "print(y_train.shape)\n",
    "\n",
    "x_test = vectorizer.transform(test_text)\n",
    "y_test = test.drop(labels = ['text', 'birthyear'], axis=1)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         fame  gender  occupation\n",
      "1689826     0       1           3\n",
      "2339612     0       1           7\n",
      "1609231     1       0           2\n",
      "521878      1       0           7\n",
      "5114930     1       1           7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "y_train = y_train.apply(LabelEncoder().fit_transform)\n",
    "print(y_train.head())\n",
    "y_test = y_test.apply(LabelEncoder().fit_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         fame  gender  occupation\n",
      "2011005     1       0           2\n",
      "3416909     1       1           2\n",
      "649605      1       0           0\n",
      "1052140     1       0           2\n",
      "1884184     1       1           7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fame</th>\n",
       "      <th>gender</th>\n",
       "      <th>occupation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011005</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3416909</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649605</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052140</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884184</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         fame  gender  occupation\n",
       "2011005     1       0           2\n",
       "3416909     1       1           2\n",
       "649605      1       0           0\n",
       "1052140     1       0           2\n",
       "1884184     1       1           7"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# reverse\n",
    "from collections import defaultdict\n",
    "df = y_train\n",
    "d = defaultdict(LabelEncoder)\n",
    "# Encoding the variable\n",
    "fit = df.apply(lambda x: d[x.name].fit_transform(x))\n",
    "\n",
    "# Inverse the encoded\n",
    "fit.apply(lambda x: d[x.name].inverse_transform(x))\n",
    "\n",
    "# Using the dictionary to label future data\n",
    "df = df.apply(lambda x: d[x.name].transform(x))\n",
    "print(df.head())\n",
    "df = y_train.apply(LabelEncoder().fit_transform)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Label Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Multiple Binary Classifications - (One Vs Rest Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fame', 'gender', 'occupation']\n",
      "vals in  fame\n",
      "[0 1 2]\n",
      "vals in  gender\n",
      "[1 0]\n",
      "vals in  occupation\n",
      "[3 7 2 0 6 1 4 5]\n"
     ]
    }
   ],
   "source": [
    "categories = list(data.columns.values)\n",
    "categories = categories[1:-1]\n",
    "print(categories)\n",
    "\n",
    "# num occps\n",
    "for categ in categories:\n",
    "    print('vals in ', categ)\n",
    "    print(y_train[categ].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Processing fame comments...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.7505456109229695\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing gender comments...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.731226146320486\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing occupation comments...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.46734397677793904\n",
      "\n",
      "\n",
      "CPU times: user 6.67 s, sys: 505 ms, total: 7.18 s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Using pipeline for applying logistic regression and one vs rest classifier\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
    "            ])\n",
    "\n",
    "for category in categories:\n",
    "    printmd('**Processing {} comments...**'.format(category))\n",
    "    \n",
    "    # Training logistic regression model on train data\n",
    "    LogReg_pipeline.fit(x_train, train[category])\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    prediction = LogReg_pipeline.predict(x_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Multiple Binary Classifications - (Binary Relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# using binary relevance\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# initialize binary relevance multi-label classifier\n",
    "# with a gaussian naive bayes base classifier\n",
    "classifier = BinaryRelevance(GaussianNB())\n",
    "\n",
    "# train\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(x_test)\n",
    "\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(y_test,predictions))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Classifier Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using classifier chains\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# initialize classifier chains multi-label classifier\n",
    "classifier = ClassifierChain(LogisticRegression())\n",
    "\n",
    "# Training logistic regression model on train datano\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(x_test)\n",
    "\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(y_test,predictions))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Label Powerset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Label Powerset\n",
    "from skmultilearn.problem_transform import LabelPowerset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# initialize label powerset multi-label classifier\n",
    "classifier = LabelPowerset(LogisticRegression())\n",
    "\n",
    "# train\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(x_test)\n",
    "\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(y_test,predictions))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
