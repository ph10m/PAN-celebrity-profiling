{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "#printmd('**bold**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file exists, continue!\n"
     ]
    }
   ],
   "source": [
    "# skip this field if \"combined_set.csv\" exists\n",
    "parsed_csv = 'combined_set.csv'\n",
    "\n",
    "if not os.path.isfile(parsed_csv):\n",
    "    feeds = pd.read_json('./feeds_2000.ndjson', lines=True)\n",
    "    labels = pd.read_json('./labels_2000.ndjson', lines=True)\n",
    "\n",
    "    # not all IDs are found in the tiny labels file, make sure we have a complete (albeit fake, dataset)\n",
    "    import random\n",
    "    valid_ids = list(feeds.id)  # the ids found in the feeds_2000\n",
    "    random.shuffle(valid_ids)  # shuffle these and assign random IDs that exist \n",
    "    labels.id = valid_ids\n",
    "    combined = pd.merge(feeds, labels, on='id')\n",
    "    expanded = combined.set_index(\n",
    "        ['id', 'birthyear', 'fame', 'gender', 'occupation']\n",
    "    )['text'].apply(pd.Series).stack()\n",
    "    expanded = expanded.reset_index()\n",
    "    expanded = expanded.drop(columns=['level_5'])  # level_5 is the auto-generated new column, containing an index\n",
    "\n",
    "    expanded.to_csv(parsed_csv)\n",
    "else:\n",
    "    print('file exists, continue!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getcwd(), parsed_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5220005, 5)\n",
      "(5219253, 5)\n"
     ]
    }
   ],
   "source": [
    "#data_raw = pd.read_csv(data_path)\n",
    "data_raw = pd.read_csv(open(data_path,'r'), encoding='utf-8', engine='c')\n",
    "data_raw.columns = ['index', 'id', 'birthyear', 'fame', 'gender', 'occupation', 'text']\n",
    "data_raw = data_raw.drop(columns=['index', 'id'])  # no need for any non-label data\n",
    "\n",
    "print(data_raw.shape)  # before dropping NaN values\n",
    "\n",
    "data_raw = data_raw.dropna()\n",
    "data_raw['birthyear'] = data_raw['birthyear'].astype(int)  # from 1978.0 -> 1978\n",
    "print(data_raw.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in data = 5219253\n",
      "Number of columns in data = 5\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Sample data:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>birthyear</th>\n",
       "      <th>fame</th>\n",
       "      <th>gender</th>\n",
       "      <th>occupation</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>Back at it with @americanidol looking for...he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>Can‚Äôt buy all the happiness in the world, it‚Äôs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>30 down @nytimes ü§ùüß°</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>üì∏ @ronyalwin üíò @ New York, New York https://t....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>üéÄ pink it was love at first sight üéÄ @ New York...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>Putting my best foot forward in The Memphis by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>Girls UNITED can never be divided! üëØ‚Äç‚ôÄÔ∏è‚ù§Ô∏è http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>BRB buying The Stephanie bow shoe as an early ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>Connect the dots, bbs  #TheClara - TeamKP @kpc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>Geometry was The Daina‚Äôs favorite subject in s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>We love a Mary June. I mean, Mary Jane. I mean...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>WE HAVE ALWAYS BEEN READY FOR THIS GELI OK üçë üçã...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>Ain‚Äôt The Turner a jewel?  üíé - TeamKP @kpcolle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>KICK üë£ BALL üë£ CHANGE in The Jo - TeamKP @kpcol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>All I want for Christmas is The Caine (sorry, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>We love The Sissy as much as we love our own s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>Live in FIVE!  -TeamKP @kpcollections #KatyPer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>T-minus one hour until I‚Äôll be on @QVC showing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>Tune in live now for a chat with @QVC! https:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>WHAT SHE SAID OK\\n    ‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è\\n\\n#Repost @r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>Incredibly moving and authentic ‚ù§Ô∏èCongrats fri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>PROUD to be covergirl for @FootwearNews &amp; even...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>I like when a lone fly catches a ride on an in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>Why didn‚Äôt the fettuccine go out for Halloween...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>Ladies, it‚Äôs time to remind them ‚úäüèª</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>RT @CoryBooker: Right, forever vigilant \\n\\nIs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>#TBT to when I got to ‚úî another one off my buc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>Proud of this üë†PLATFORM üë† (yes I did!) to rais...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>*BLOCKS* https://t.co/ntOG1A7ede</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1991</td>\n",
       "      <td>star</td>\n",
       "      <td>male</td>\n",
       "      <td>performer</td>\n",
       "      <td>I just cracked my molar in half on a ranch cor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    birthyear  fame gender occupation  \\\n",
       "0        1991  star   male  performer   \n",
       "1        1991  star   male  performer   \n",
       "2        1991  star   male  performer   \n",
       "3        1991  star   male  performer   \n",
       "4        1991  star   male  performer   \n",
       "5        1991  star   male  performer   \n",
       "6        1991  star   male  performer   \n",
       "7        1991  star   male  performer   \n",
       "8        1991  star   male  performer   \n",
       "9        1991  star   male  performer   \n",
       "10       1991  star   male  performer   \n",
       "11       1991  star   male  performer   \n",
       "12       1991  star   male  performer   \n",
       "13       1991  star   male  performer   \n",
       "14       1991  star   male  performer   \n",
       "15       1991  star   male  performer   \n",
       "16       1991  star   male  performer   \n",
       "17       1991  star   male  performer   \n",
       "18       1991  star   male  performer   \n",
       "19       1991  star   male  performer   \n",
       "20       1991  star   male  performer   \n",
       "21       1991  star   male  performer   \n",
       "22       1991  star   male  performer   \n",
       "23       1991  star   male  performer   \n",
       "24       1991  star   male  performer   \n",
       "25       1991  star   male  performer   \n",
       "26       1991  star   male  performer   \n",
       "27       1991  star   male  performer   \n",
       "28       1991  star   male  performer   \n",
       "29       1991  star   male  performer   \n",
       "\n",
       "                                                 text  \n",
       "0   Back at it with @americanidol looking for...he...  \n",
       "1   Can‚Äôt buy all the happiness in the world, it‚Äôs...  \n",
       "2                                 30 down @nytimes ü§ùüß°  \n",
       "3   üì∏ @ronyalwin üíò @ New York, New York https://t....  \n",
       "4   üéÄ pink it was love at first sight üéÄ @ New York...  \n",
       "5   Putting my best foot forward in The Memphis by...  \n",
       "6   Girls UNITED can never be divided! üëØ‚Äç‚ôÄÔ∏è‚ù§Ô∏è http...  \n",
       "7   BRB buying The Stephanie bow shoe as an early ...  \n",
       "8   Connect the dots, bbs  #TheClara - TeamKP @kpc...  \n",
       "9   Geometry was The Daina‚Äôs favorite subject in s...  \n",
       "10  We love a Mary June. I mean, Mary Jane. I mean...  \n",
       "11  WE HAVE ALWAYS BEEN READY FOR THIS GELI OK üçë üçã...  \n",
       "12  Ain‚Äôt The Turner a jewel?  üíé - TeamKP @kpcolle...  \n",
       "13  KICK üë£ BALL üë£ CHANGE in The Jo - TeamKP @kpcol...  \n",
       "14  All I want for Christmas is The Caine (sorry, ...  \n",
       "15  We love The Sissy as much as we love our own s...  \n",
       "16  Live in FIVE!  -TeamKP @kpcollections #KatyPer...  \n",
       "17  T-minus one hour until I‚Äôll be on @QVC showing...  \n",
       "18  Tune in live now for a chat with @QVC! https:/...  \n",
       "19  WHAT SHE SAID OK\\n    ‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è\\n\\n#Repost @r...  \n",
       "20  Incredibly moving and authentic ‚ù§Ô∏èCongrats fri...  \n",
       "21  PROUD to be covergirl for @FootwearNews & even...  \n",
       "22  I like when a lone fly catches a ride on an in...  \n",
       "23  Why didn‚Äôt the fettuccine go out for Halloween...  \n",
       "24                Ladies, it‚Äôs time to remind them ‚úäüèª  \n",
       "25  RT @CoryBooker: Right, forever vigilant \\n\\nIs...  \n",
       "26  #TBT to when I got to ‚úî another one off my buc...  \n",
       "27  Proud of this üë†PLATFORM üë† (yes I did!) to rais...  \n",
       "28                   *BLOCKS* https://t.co/ntOG1A7ede  \n",
       "29  I just cracked my molar in half on a ranch cor...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of rows in data =\",data_raw.shape[0])\n",
    "print(\"Number of columns in data =\",data_raw.shape[1])\n",
    "print(\"\\n\")\n",
    "printmd(\"**Sample data:**\")\n",
    "data_raw.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['birthyear', 'fame', 'gender', 'occupation']\n"
     ]
    }
   ],
   "source": [
    "categories = list(data_raw.columns.values)\n",
    "categories = categories[:-1]\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_raw\n",
    "data = data_raw.loc[np.random.choice(data_raw.index, size=10000)]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanHtml(sentence):\n",
    "    #cleanr = re.compile('<.*?>')\n",
    "    #cleantext = re.sub(cleanr, ' ', str(sentence))\n",
    "    cleantext = re.sub(r'http\\S+', '', sentence)\n",
    "    return cleantext\n",
    "\n",
    "def cleanRetweet(sentence):\n",
    "    return re.sub(r'rt', '', sentence)\n",
    "\n",
    "def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "\n",
    "def cleanMentions(sentence):\n",
    "    return re.sub(r'@#?\\b\\w\\w+\\b', '', sentence)\n",
    "\n",
    "def keepAlphaHash(sentence):\n",
    "    return ' '.join([w for w in sentence.split() if w.isalpha() or '#' in w])\n",
    "\n",
    "def keepAlpha(sentence):\n",
    "    alpha_sent = \"\"\n",
    "    for word in sentence.split():\n",
    "        alpha_word = re.sub('[^# a-z A-Z]+', ' ', word)\n",
    "        alpha_sent += alpha_word\n",
    "        alpha_sent += \" \"\n",
    "    alpha_sent = alpha_sent.strip()\n",
    "    return alpha_sent\n",
    "\n",
    "def emptyToNan(sentence):\n",
    "    if len(sentence) < 1:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].str.lower()\n",
    "data['text'] = data['text'].apply(cleanRetweet)\n",
    "data['text'] = data['text'].apply(cleanHtml)\n",
    "data['text'] = data['text'].apply(cleanPunc)\n",
    "data['text'] = data['text'].apply(keepAlphaHash)\n",
    "\n",
    "# prune empty sentences, replace with NaN and use the built-in dropna() func\n",
    "data['text'] = data['text'].apply(emptyToNan)\n",
    "data = data.dropna()\n",
    "data.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])\n",
    "re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\n",
    "def removeStopWords(sentence):\n",
    "    global re_stop_words\n",
    "    return re_stop_words.sub(\" \", sentence)\n",
    "\n",
    "data['text'] = data['text'].apply(removeStopWords)\n",
    "data.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "\n",
    "data['text'] = data['text'].apply(stemming)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, random_state=42, test_size=0.10, shuffle=True)\n",
    "\n",
    "train.to_csv('formatted/train.csv')\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train['text']\n",
    "test_text = test['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\n",
    "vectorizer.fit(train_text)\n",
    "vectorizer.fit(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorizer.transform(train_text)\n",
    "y_train = train.drop(labels = ['text'], axis=1)\n",
    "\n",
    "x_test = vectorizer.transform(test_text)\n",
    "y_test = test.drop(labels = ['text'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Label Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Multiple Binary Classifications - (One Vs Rest Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Using pipeline for applying logistic regression and one vs rest classifier\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
    "            ])\n",
    "\n",
    "for category in categories:\n",
    "    printmd('**Processing {} comments...**'.format(category))\n",
    "    \n",
    "    # Training logistic regression model on train data\n",
    "    LogReg_pipeline.fit(x_train, train[category])\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    prediction = LogReg_pipeline.predict(x_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Multiple Binary Classifications - (Binary Relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# using binary relevance\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# initialize binary relevance multi-label classifier\n",
    "# with a gaussian naive bayes base classifier\n",
    "classifier = BinaryRelevance(GaussianNB())\n",
    "\n",
    "# train\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(x_test)\n",
    "\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(y_test,predictions))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Classifier Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using classifier chains\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# initialize classifier chains multi-label classifier\n",
    "classifier = ClassifierChain(LogisticRegression())\n",
    "\n",
    "# Training logistic regression model on train data\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(x_test)\n",
    "\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(y_test,predictions))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Label Powerset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Label Powerset\n",
    "from skmultilearn.problem_transform import LabelPowerset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# initialize label powerset multi-label classifier\n",
    "classifier = LabelPowerset(LogisticRegression())\n",
    "\n",
    "# train\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(x_test)\n",
    "\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(y_test,predictions))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
